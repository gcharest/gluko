# Release v0.3: Shard-Based Dataset Loading

## Release Summary

Implement NDJSON-based dataset sharding with manifest versioning and incremental updates, enabling efficient dataset management without requiring re-download of entire dataset when data hasn't changed.

## Goals

1. **Manifest-based versioning**: Ship a manifest file listing all shards with checksums and versions
2. **Incremental updates**: Only download shards that changed since last sync
3. **Efficient storage**: Split dataset into reasonably-sized chunks (target 500 KB compressed each)
4. **Validation**: Verify shard integrity via checksums before committing to IndexedDB
5. **Resumability**: Support resuming interrupted downloads

## Constraints Specific to This Release

- **Static file hosting**: No backend; dataset served as static files (GitHub Pages or CDN)
- **Compression**: Shards should be pre-compressed (gzip/brotli) for efficient transfer
- **Mobile networks**: Must work on 3G/LTE with potential interruptions
- **Storage limits**: Target devices may have < 100 MB available for dataset
- **No breaking changes**: Must not break existing local data from v0.1-v0.2

## Options Considered

### Option A: Single file dataset

- **Approach**: Serve entire dataset as one monolithic NDJSON file
- **Pros**: Simple, no shard coordination needed
- **Cons**: 5+ MB re-download every time, poor for mobile
- **Risk**: Low (currently working) but fails product goals

### Option B: Manifest-based sharding with fixed shard count

- **Approach**: Pre-determine shard count (e.g., 20 shards of ~500 KB each)
- **Pros**: Simple manifest, predictable structure
- **Cons**: Wasteful if dataset grows unevenly
- **Risk**: Low

### Option C: Manifest-based sharding with dynamic shard count

- **Approach**: Re-shard dataset as needed; manifest lists current shards dynamically
- **Pros**: Efficient storage, adapts to dataset growth
- **Cons**: More complex tooling, potential alignment issues
- **Risk**: Medium (more complex, but beneficial long-term)

## Selected: Option C

**Why**: ETL process should determine optimal shard boundaries based on actual data; manifest reflects reality. As dataset grows, shards can be resized without breaking client code. More investment in tooling pays off over time.

## Success Criteria

- [ ] Manifest file generated correctly by ETL with all shards listed
- [ ] App downloads all shards on first run
- [ ] Shard checksums validated before storage
- [ ] On unchanged manifest, zero downloads on repeat visits (use cached data)
- [ ] On manifest change, only changed shards downloaded
- [ ] Dataset fully searchable after import
- [ ] Interrupted download can resume from last checkpoint
- [ ] Works on low-end devices (< 4 GB RAM)

## Known Risks

- **Shard size variability**: ETL sharding algorithm may produce uneven sizes → Monitor and tune algorithm
- **Network interruptions**: Download can fail mid-shard → Implement per-shard progress tracking and resumability
- **Storage quota**: Full dataset won't fit on some devices → Graceful error and option to keep partial dataset

## Out of Scope

- Service Worker caching optimization (v0.4)
- Streaming import with workers (v0.4)
- Per-shard priority/lazy loading
- Background incremental sync

## Related Documentation

- **Previous release**: [v0.2-pwa-installability.md](v0.2-pwa-installability.md)
- **Release tracker**: [RELEASE_TRACKER.md](RELEASE_TRACKER.md)
- **Next release**: [v0.4-pwa-optimization.md](v0.4-pwa-optimization.md)
- Product goals: [PRODUCT.md](../PRODUCT.md)
- Architecture: [ARCHITECTURE.md](../ARCHITECTURE.md)
