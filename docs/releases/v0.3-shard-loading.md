# Release v0.3: Shard-Based Dataset Loading

## Release Summary

Implement NDJSON-based dataset sharding with manifest versioning and incremental updates, enabling efficient dataset management without requiring re-download of entire dataset when data hasn't changed.

## Goals

1. **Manifest-based versioning**: Ship a manifest file listing all shards with checksums and versions
2. **Incremental updates**: Only download shards that changed since last sync
3. **Efficient storage**: Split dataset into reasonably-sized chunks (target 500 KB compressed each)
4. **Validation**: Verify shard integrity via checksums before committing to IndexedDB
5. **Resumability**: Support resuming interrupted downloads

## Constraints Specific to This Release

- **Static file hosting**: No backend; dataset served as static files (GitHub Pages or CDN)
- **Compression**: Shards should be pre-compressed (gzip/brotli) for efficient transfer
- **Mobile networks**: Must work on 3G/LTE with potential interruptions
- **Storage limits**: Target devices may have < 100 MB available for dataset
- **No breaking changes**: Must not break existing local data from v0.1-v0.2
- **Storage quota management**: Proactively check available space before large imports per 2025 offline-first patterns
- **Transaction atomicity**: Each shard write must be atomic (all records or none) to prevent partial shard state

## Options Considered

### Option A: Single file dataset

- **Approach**: Serve entire dataset as one monolithic NDJSON file
- **Pros**: Simple, no shard coordination needed
- **Cons**: 5+ MB re-download every time, poor for mobile
- **Risk**: Low (currently working) but fails product goals

### Option B: Manifest-based sharding with fixed shard count

- **Approach**: Pre-determine shard count (e.g., 20 shards of ~500 KB each)
- **Pros**: Simple manifest, predictable structure
- **Cons**: Wasteful if dataset grows unevenly
- **Risk**: Low

### Option C: Manifest-based sharding with dynamic shard count

- **Approach**: Re-shard dataset as needed; manifest lists current shards dynamically
- **Pros**: Efficient storage, adapts to dataset growth
- **Cons**: More complex tooling, potential alignment issues
- **Risk**: Medium (more complex, but beneficial long-term)

## Selected: Option C

**Why**: ETL process should determine optimal shard boundaries based on actual data; manifest reflects reality. As dataset grows, shards can be resized without breaking client code. More investment in tooling pays off over time.

## Storage Quota & Transaction Safety (2025 Best Practices)

### Proactive Quota Checking

Before downloading shards, check if sufficient storage space available:

```typescript
async function checkStorageQuota(): Promise<{ available: number }> {
  if (!navigator.storage?.estimate) {
    return { available: Infinity }  // Assume sufficient if API not available
  }

  const quota = await navigator.storage.estimate()
  const available = quota.quota - quota.usage
  const percentUsed = Math.round((quota.usage / quota.quota) * 100)

  if (percentUsed > 90) {
    showWarning(`Storage ${percentUsed}% full. Free space before downloading dataset.`)
  }

  if (available < expectedShardSize) {
    showError('Insufficient storage. Please clear some data first.')
    return { available: 0 }
  }

  return { available }
}
```

### Atomic Shard Import

Each shard download + IndexedDB write must be **atomic** to prevent partial shard state:

```typescript
async function importShardAtomically(shard: ShardData, shardId: string): Promise<void> {
  // Validate shard structure BEFORE writing
  const validated = validateShard(shard)
  if (!validated) {
    throw new Error(`Invalid shard structure: ${shardId}`)
  }

  // Single transaction: all records or none
  return new Promise((resolve, reject) => {
    const transaction = db.transaction(['nutrients'], 'readwrite')

    // Write all records synchronously
    for (const item of validated.items) {
      transaction.objectStore('nutrients').add(item)
    }

    transaction.oncomplete = () => resolve()
    transaction.onerror = () => reject(transaction.error)
    transaction.onabort = () => reject(new Error('Shard import aborted'))
  })
}
```

**Why**: If browser crashes during shard write, entire shard is rolled back automatically. Prevents "half-imported" shard state that corrupts dataset.

### Handle QuotaExceededError

Gracefully handle storage exhaustion during import:

```typescript
try {
  await importShardAtomically(shard, shardId)
} catch (error) {
  if (error.name === 'QuotaExceededError') {
    showError('Storage full. Export old meals or clear unused data.')
    // Offer cleanup options:
    // 1. Export history to file (v0.6)
    // 2. Delete meals older than 6 months
    // 3. Keep partial dataset (mark incomplete)
    return false
  }
  throw error
}
```

### UI Requirements for v0.3

- Display current storage usage percentage in settings
- Warn user when > 80% capacity reached
- Show pre-import quota check before shard download starts
- Offer cleanup options if quota insufficient
- Show progress during multi-shard import
- Gracefully handle mid-import failures with resumability

## Success Criteria

- [ ] Manifest file generated correctly by ETL with all shards listed
- [ ] App downloads all shards on first run
- [ ] Shard checksums validated before storage
- [ ] On unchanged manifest, zero downloads on repeat visits (use cached data)
- [ ] On manifest change, only changed shards downloaded
- [ ] Dataset fully searchable after import
- [ ] Interrupted download can resume from last checkpoint
- [ ] Works on low-end devices (< 4 GB RAM)

## Known Risks

- **Shard size variability**: ETL sharding algorithm may produce uneven sizes → Monitor and tune algorithm
- **Network interruptions**: Download can fail mid-shard → Implement per-shard progress tracking and resumability
- **Storage quota**: Full dataset won't fit on some devices → Graceful error and option to keep partial dataset

## Out of Scope

- Service Worker caching optimization (v0.4)
- Streaming import with workers (v0.4)
- Per-shard priority/lazy loading
- Background incremental sync

## Related Documentation

- **Previous release**: [v0.2-pwa-installability.md](v0.2-pwa-installability.md)
- **Release tracker**: [RELEASE_TRACKER.md](RELEASE_TRACKER.md)
- **Next release**: [v0.4-pwa-optimization.md](v0.4-pwa-optimization.md)
- Product goals: [PRODUCT.md](../PRODUCT.md)
- Architecture: [ARCHITECTURE.md](../ARCHITECTURE.md)
